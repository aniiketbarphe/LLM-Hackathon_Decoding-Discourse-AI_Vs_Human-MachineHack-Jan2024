![ROBO](https://github.com/aniiketbarphe/LLM-Hackathon_Decoding-Discourse-AI_Vs_Human-MachineHack-Jan2024/assets/84449238/953ea060-27d3-4bff-a2cd-9a77f243ea1f)

# LLM-Hackathon_Decoding-Discourse-AI_Vs_Human-MachineHack-Jan2024

![MH-EMAIL](https://github.com/aniiketbarphe/LLM-Hackathon_Decoding-Discourse-AI_Vs_Human-MachineHack-Jan2024/assets/84449238/005a78a3-a776-4ea1-bb39-265208585077)

#

**1) Problem Statement:-**

Welcome to an exhilarating hackathon where participants will immerse themselves in the realm of natural language processing (NLP) and essay evaluation. In this challenge, participants will be tasked with distinguishing between essays crafted by students and those generated by a Language Model (LLM). 

#

**2) About the Dataset:-**

**2.1) Data Dictionary:-**

a) The training dataset comprises 212 unique records, each identified by an 'id,' and participants will utilize information from 'prompt_id,' 'text,' and 'generated' fields to develop models that accurately predict the origin of each essay.

b) The test dataset, consisting of 188 records, lacks the 'generated' field, presenting the ultimate test for participants to assess the generalization of their models. The crux of the competition lies in comprehending and analyzing the context of the essays, as the training prompts are provided separately in the 'train_prompts.csv' file. With 'prompt_id' serving as a link between essays and prompts, participants will explore the 'prompt_name,' 'instructions,' and 'source_text' fields to gain insights into the background information influencing essay writing.

**2.2) Data description:-** The train dataset contains 212 records and the test data contains 188 records. Here is the description of the data:-

**2.2.1)** train.csv and test.csv

**a)id:-** A unique identifier for each essay.

**b)prompt_id:-** Identifies the prompt the essay was written in response to.

**c)text:-** The essay text.

**d)generated:-** Whether the essay was written by a student (1) or generated by an LLM (0). This field is the target and is not present in test.csv.

**2.2.2)** train_prompts.csv - Essays were written in response to information in these fields.

**a)prompt_id:-** A unique identifier for each prompt.

**b)prompt_name:-** The title of the prompt.

**c)instructions:-** The instructions given to students.

**d)source_text:-** The text of the article(s) the essays were written in response to, in Markdown format. Each article is preceded with its title in a heading, like # Title. When an author is indicated, their name will be given in the title after by. Not all articles have authors indicated. An article may have subheadings indicated like ## Subheading.

**2.2.3)** sample_submission.csv - A submission file in the correct format.
#

**3) Evaluation metric:-** All submissions will be evaluated using the roc_auc metric. 
#
**4) Public and Private Split:-**

a) This competition supports private and public leaderboards

b) The public leaderboard is evaluated on 30% of Test data

c)The private leaderboard will be evaluated on 100% of the Test data
#
**5) Prizes:-**

üèÖ The top 3 winners will get MLDS 2024 passes.

# Summary

**a) Best Submission Score:-**

**1) Public Leaderboard :-** 0.97222

**2) Private Leaderboard :-** 1.00000
#
**b) Best Model:-** [ **Ensemble Technique** (Logistic Regression + Random Forest Classifier +Stochastic Gradient Descent Classifier (Generate Probability Estimates))]

![Aniket-private-lb](https://github.com/aniiketbarphe/LLM-Hackathon_Decoding-Discourse-AI_Vs_Human-MachineHack-Jan2024/assets/84449238/7f6b192f-e639-4b33-a929-6b0de5c9e914)

![Aniket-public-LB](https://github.com/aniiketbarphe/LLM-Hackathon_Decoding-Discourse-AI_Vs_Human-MachineHack-Jan2024/assets/84449238/d2fbad5d-cd50-49c5-8597-45fd34fc28ad)
#
**c) Competition Link:-** https://machinehack.com/hackathons/llm_hackathon_decoding_discourse_ai_vs_human/leaderboard
#
**d) Rank Scored:-**

d1) **28** out of **296** registered participants (Private Leaderboard)

d2) **25** out of **296** registered participants (Public Leaderboard)
