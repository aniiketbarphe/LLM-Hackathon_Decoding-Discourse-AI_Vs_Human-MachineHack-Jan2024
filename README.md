![ROBO](https://github.com/aniiketbarphe/LLM-Hackathon_Decoding-Discourse-AI_Vs_Human-MachineHack-Jan2024/assets/84449238/953ea060-27d3-4bff-a2cd-9a77f243ea1f)

# LLM-Hackathon_Decoding-Discourse-AI_Vs_Human-MachineHack-Jan2024

![MH-EMAIL](https://github.com/aniiketbarphe/LLM-Hackathon_Decoding-Discourse-AI_Vs_Human-MachineHack-Jan2024/assets/84449238/005a78a3-a776-4ea1-bb39-265208585077)


Welcome to an exhilarating hackathon where participants will immerse themselves in the realm of natural language processing (NLP) and essay evaluation. In this challenge, participants will be tasked with distinguishing between essays crafted by students and those generated by a Language Model (LLM). The training dataset comprises 212 unique records, each identified by an 'id,' and participants will utilize information from 'prompt_id,' 'text,' and 'generated' fields to develop models that accurately predict the origin of each essay.

 

The test dataset, consisting of 188 records, lacks the 'generated' field, presenting the ultimate test for participants to assess the generalization of their models. The crux of the competition lies in comprehending and analyzing the context of the essays, as the training prompts are provided separately in the 'train_prompts.csv' file. With 'prompt_id' serving as a link between essays and prompts, participants will explore the 'prompt_name,' 'instructions,' and 'source_text' fields to gain insights into the background information influencing essay writing.

 

As participants navigate this hackathon, they are encouraged to approach the task with creativity, innovation, and a deep understanding of both the intricacies of natural language and the context provided by the prompts. Crafting a successful solution will require a fusion of advanced machine learning techniques and a keen appreciation for the nuances of human expression. 

 

Participants are required to leverage any LLM to showcase the capabilities of Generative AI in distinguishing whether an essay is generated by a human or AI.

The train dataset contains 212 records and the test data contains 188 records. Here is the description of the data.

train|test.csv

id : A unique identifier for each essay.
prompt_id : Identifies the prompt the essay was written in response to.
text : The essay text.
generated : Whether the essay was written by a student (1) or generated by an LLM (0). This field is the target and is not present in test.csv.
train_prompts.csv - Essays were written in response to information in these fields.

prompt_id - A unique identifier for each prompt.
prompt_name - The title of the prompt.
instructions - The instructions given to students.
source_text - The text of the article(s) the essays were written in response to, in Markdown format. Each article is preceded with its title in a heading, like # Title. When an author is indicated, their name will be given in the title after by. Not all articles have authors indicated. An article may have subheadings indicated like ## Subheading.
sample_submission.csv - A submission file in the correct format.

What is the Metric In this competition? How is the Leaderboard Calculated?

All submissions will be evaluated using the roc_auc metric. One can use sklearn.metrics.roc_auc_score to get a valid score
This competition supports private and public leaderboards
The public leaderboard is evaluated on 30% of Test data
The private leaderboard will be evaluated on 100% of the Test data
The Final Score represents the score achieved based on the Best Score on the public leaderboard
How to Generate a Valid Submission File
The participant should submit a .csv file. The submission will return an Invalid Score if you have extra rows or columns
Participants are required to leverage any LLM to showcase the capabilities of Generative AI in distinguishing whether an essay is generated by a human or AI.
Note:

Do not shuffle the sequence of the test series
If you are using pandas, use this submission code: 

Hackathon Specific Rules
This private leaderboard will be freezed on 26 January 2024 at 06:00 PM IST.

Disqualification

If any of the details entered are found incorrect, Analytics India Magazine and MachineHack reserve the right to disqualify any participant.
Any external dataset usage is strictly prohibited. The participants will be disqualified if found using any external dataset.
One Account Per Participant
The hackathon will allow only one account per participant.
 Submissions from multiple accounts will lead to disqualification.
All registered users are eligible to participate in the hackathon.
No Private Sharing
Privately sharing code or data outside of teams is not permitted.
 It's okay to share code if made available to all participants on the forums.
Submission Limits
The hackathon has a submission limit of 3 per day, after which the submissions will not be considered or evaluated.
All registered users are eligible to participate in the hackathon. We ask that you respect the spirit of the competition and do not cheat.

MLDS 2024 Passes IN PRIZES

The top 3 winners will get MLDS 2024 passes.

![Aniket-private-lb](https://github.com/aniiketbarphe/LLM-Hackathon_Decoding-Discourse-AI_Vs_Human-MachineHack-Jan2024/assets/84449238/7f6b192f-e639-4b33-a929-6b0de5c9e914)

![Aniket-public-LB](https://github.com/aniiketbarphe/LLM-Hackathon_Decoding-Discourse-AI_Vs_Human-MachineHack-Jan2024/assets/84449238/d2fbad5d-cd50-49c5-8597-45fd34fc28ad)
